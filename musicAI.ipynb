{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install MAESTRO",
   "id": "329f050c8ccf9c55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!wget https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0.zip\n",
    "!unzip maestro-v3.0.0.zip"
   ],
   "id": "5fb383854978b8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install dependencies ",
   "id": "dc0d344b87d4c663"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T18:50:21.175030Z",
     "start_time": "2025-03-10T18:50:15.660208Z"
    }
   },
   "source": "!pip install torch transformers pretty_midi numpy tqdm",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.45.1)\n",
      "Requirement already satisfied: pretty_midi in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.10)\n",
      "Requirement already satisfied: numpy in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: mido>=1.1.16 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pretty_midi) (1.3.3)\n",
      "Requirement already satisfied: six in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pretty_midi) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bulatypov\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import modules",
   "id": "fc31a943e348a246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T18:52:53.357975Z",
     "start_time": "2025-03-10T18:52:53.351240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, GPT2Model, GPT2Config\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer"
   ],
   "id": "62b72d9742e34766",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hypothetical model architecture",
   "id": "93bf8c571d020d45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T18:52:54.629067Z",
     "start_time": "2025-03-10T18:52:54.620195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConditionalMusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, text_embed_dim=256, music_embed_dim=512, n_head=8):\n",
    "        super().__init__()\n",
    "        self.text_encoder = GPT2Model(GPT2Config(\n",
    "            vocab_size=30522,\n",
    "            n_embd=text_embed_dim,\n",
    "            n_head=n_head,\n",
    "            n_layer=4\n",
    "        ))\n",
    "        self.music_embed = nn.Embedding(vocab_size, music_embed_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=music_embed_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=4,\n",
    "            num_decoder_layers=4\n",
    "        )\n",
    "        self.text_proj = nn.Linear(text_embed_dim, music_embed_dim)\n",
    "        self.note_predictor = nn.Linear(music_embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, music_seq, text_input, mask=None):\n",
    "        text_features = self.text_encoder(**text_input).last_hidden_state.mean(1)\n",
    "        text_features = self.text_proj(text_features)\n",
    "        music_emb = self.music_embed(music_seq)\n",
    "        music_emb = music_emb + text_features.unsqueeze(1)\n",
    "        output = self.transformer(music_emb, music_emb, tgt_mask=mask)\n",
    "        return self.note_predictor(output)"
   ],
   "id": "a297098feae0b346",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "c43c3ccd6dcb85b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T18:52:56.284972Z",
     "start_time": "2025-03-10T18:52:56.162047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAESTRO_PATH = \"maestro-v3.0.0\"\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 512\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(MAESTRO_PATH, \"maestro-v3.0.0.csv\"))\n",
    "midi_files = [os.path.join(MAESTRO_PATH, row.midi_filename) for _, row in metadata.iterrows()]"
   ],
   "id": "a59359eacaeb10e9",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T18:52:57.166110Z",
     "start_time": "2025-03-10T18:52:57.152640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaestroDataset(Dataset):\n",
    "    def __init__(self, midi_files, metadata, seq_length=512):\n",
    "        self.midi_files = midi_files\n",
    "        self.metadata = metadata\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Musical note vocabulary\n",
    "        self.vocab = {\n",
    "            **{i: i for i in range(128)},  # Notes\n",
    "            **{128+i: i for i in range(4)} # Durations (0.25, 0.5, 1.0, 2.0 sec)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.midi_files)\n",
    "\n",
    "    def _quantize_duration(self, duration):\n",
    "        quantized = round(duration * 4) / 4\n",
    "        return min(quantized, 2.0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        midi_data = pretty_midi.PrettyMIDI(self.midi_files[idx])\n",
    "\n",
    "        notes = []\n",
    "        for instrument in midi_data.instruments:\n",
    "            for note in instrument.notes:\n",
    "                duration = self._quantize_duration(note.end - note.start)\n",
    "                notes.append((note.pitch, duration))\n",
    "\n",
    "        bpm = int(self.metadata.iloc[idx].tempo)\n",
    "        tempo_desc = \"fast\" if bpm > 120 else \"moderate\" if bpm > 80 else \"slow\"\n",
    "        text = f\"classical piano {tempo_desc} tempo\"\n",
    "        text_tokens = self.tokenizer(\n",
    "            text,\n",
    "            max_length=32,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        seq = []\n",
    "        for pitch, duration in notes[:self.seq_length]:\n",
    "            seq.append(pitch)\n",
    "            seq.append(128 + int(duration * 4))\n",
    "\n",
    "        if len(seq) < self.seq_length:\n",
    "            seq += [0] * (self.seq_length - len(seq))\n",
    "\n",
    "        return {\n",
    "            'input_seq': torch.LongTensor(seq[:-1]),\n",
    "            'target_seq': torch.LongTensor(seq[1:]),\n",
    "            'text_input': text_tokens\n",
    "        }\n"
   ],
   "id": "44effc86bb7d9cb2",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model training",
   "id": "52b4a1dcaf6cc3de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = MaestroDataset(midi_files, metadata, SEQ_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = ConditionalMusicTransformer(vocab_size=132)  # The size: 128 notes + 4 durations\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for batch in dataloader:\n",
    "        outputs = model(\n",
    "            batch['input_seq'],\n",
    "            batch['text_input']\n",
    "        )\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            outputs.view(-1, 132),\n",
    "            batch['target_seq'].view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")"
   ],
   "id": "744c99efd53ae2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Music generation code with example",
   "id": "b3945b63b0b00a5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T18:50:29.528438Z",
     "start_time": "2025-03-10T18:50:29.527437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_from_prompt(model, prompt, max_length=512):\n",
    "    model.eval()\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    text_input = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    current_seq = torch.LongTensor([[60, 130]])\n",
    "\n",
    "    for _ in range(max_length//2):\n",
    "        with torch.no_grad():\n",
    "            output = model(current_seq, text_input)\n",
    "            next_pitch = torch.argmax(output[0, -2:-1])\n",
    "            next_duration = torch.argmax(output[0, -1:])\n",
    "            current_seq = torch.cat([\n",
    "                current_seq,\n",
    "                torch.LongTensor([[next_pitch, next_duration]])\n",
    "            ], dim=1)\n",
    "\n",
    "    return current_seq.squeeze().tolist()\n",
    "\n",
    "# Example\n",
    "generated = generate_from_prompt(model, \"classical piano fast tempo\")"
   ],
   "id": "80cb685111a335e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save to MIDI",
   "id": "10e264ece822e632"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T18:50:29.531441Z",
     "start_time": "2025-03-10T18:50:29.530436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def seq_to_midi(seq, filename):\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    piano = pretty_midi.Instrument(program=0)\n",
    "\n",
    "    time = 0.0\n",
    "    for i in range(0, len(seq)-1, 2):\n",
    "        pitch = seq[i]\n",
    "        duration = (seq[i+1] - 128) * 0.25 if seq[i+1] >= 128 else 0.25\n",
    "\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=100,\n",
    "            pitch=pitch,\n",
    "            start=time,\n",
    "            end=time + duration\n",
    "        )\n",
    "        piano.notes.append(note)\n",
    "        time += duration\n",
    "\n",
    "    midi.instruments.append(piano)\n",
    "    midi.write(filename)\n",
    "\n",
    "seq_to_midi(generated, \"generated.mid\")"
   ],
   "id": "15516ca8f2eb7ffe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
